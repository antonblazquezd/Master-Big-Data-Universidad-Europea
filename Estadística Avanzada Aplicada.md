# Estad칤stica Avanzada Aplicada

Elena Igualada Villodre
    - Ingeniera Industrial 
    - Doctora en Matem치ticas


4 actividades diferentes (10%)
    - 1. Cuestionario tipo test. 
    - 3. Pr치cticas en Python. 


5 semanas de asignatura
Examen en marzo 


# Contenido del Curso

- Objetivos
    - Objetivos espec칤ficos
    - Resultados de aprendizaje
- Unidad 1. Variables y Estimadores
    - Teor칤a de probabilidad y estad칤stica b치sica
    - T칠cnicas de descripci칩n y modelado de datos complejos: muestreo, training, test set
    - Introducci칩n al aprendizaje estad칤stico: inferencia, aprendizaje bayesiano, funci칩n de p칠rdida
- Unidad 2. Modelos de Regresi칩n y regularizaci칩n
    - Regresi칩n lineal, no lineal y log칤stica
    - Regresi칩n con m칰ltiples variables
    - Regularizaci칩n: ridge y lasso
    - Regresi칩n no param칠trica: spline y kernel.
Unidad 3. An치lisis y modelado avanzados
    - 츼rboles de decisi칩n. Modelos gr치ficos probabilistas.
    - Modelado de funciones de densidad de probabilidad
    - An치lisis de Series temporales
    - Optimizaci칩n para grandes vol칰menes de datos

# Objetivos

## Objetivos espec칤ficos
- Dise침ar y aplicar algoritmos de an치lisis basados en sistemas e infraestructuras de almacenamiento y acceso a grandes vol칰menes de datos. 

## Resultados de aprendizaje

- Comparaci칩n y contraste cr칤ticos de m칰ltiples modelos de sistemas distribuidos y sus tecnolog칤as habilitantes asociadas.

# Unidad 1. Variables y Estimadores

## Probabilidad y variables estimatorias

- Espacio muestral: Al conjunto de todos los escenarios posibles de un experimento .
- Suceso elemental: Cada uno de los posibles resultados de un experimento contenido en un espacio muestral. 
- Suceso compuesto: subconjunto del espacio muestral o una agrupaci칩n de espacios muestrales. 
- Sucesos disjuntos o mutuamente excluyentes: si pasa uno el otro no puede pasar. 
- Enfoque frecuentista de la probabilidad: cree que la probabilidad de que los sucesos elementales sucedan es igual. Cree que en la tendencia a un resultado si lo repetimos una y otra vez. 
- Enfoque axiom치tico: cree que los sucesos elementales no son igualmente probables de ocurrir. Considera en que la suma de todas las probabilidades de los sucesos elementales debe ser igual a 1. 
- Variable aleatoria: mide una propiedad de la 
- Variables aleatorias discretas --> funci칩n de masa 
- Variables aleatorias continuas --> funci칩n de densidad  
- Modelos probabil칤sticos t칤picos: 
	- Binomial: Dos sucesos elementales: 칠xito/fracaso.
	- Poisson: modelar las probabilidades un tiempo determinado 
	- Exponencial 
	- Uniforme
	-  Modelo normal: sigue el teorema central del l칤mite. 

# 2. Modelos de Regresi칩n y regularizaci칩n 

## Teor칤a de muestreo y partici칩n de datos

- Metodolog칤a para el an치lisis de datos: 
	- Preparaci칩n de los datos
		- Recogida
		- Limpieza de los datos (eliminar datos duplicados, errores, campos vac칤os y nulos).
		- Transformaci칩n de variables categ칩ricas en num칠ricas. 
	- An치lisis estad칤stico
		- Estad칤stica descriptiva: Aplicamos un an치lisis exploratorio de datos (EDA), sin preguntarnos por la poblaci칩n ni hacer inferenc. 
		- An치lisis estad칤stico: se buscan modelos que se ajusten a nuestros datos
		- Aprendizaje sobre el fen칩meno
		- Creamos un modelo que sea capaz de predecir el comportamiento de una variable futura. 
Teor칤a de muestreo 
	- Muestra vs. Poblaci칩n 
		- Poblaci칩n -objetivo: es un subconjunto del universo que engloba a toda la poblaci칩n que cumple las condiciones para realizar un experimento. 
		- Muestra: una parte representativa de la poblaci칩n. 
	- Distribuci칩n muestral 
		- Necesitamos conocer el nivel de incertidumbre de nuestros estad칤sticos y para ello debemos conocer la distribuci칩n muestral
	- Par치metros vs Estad칤stico
		- Par치metro: es una medida descriptiva de una poblaci칩n
		- Estad칤stico: es una medida descriptiva de la muestra. 
- Inferencia estad칤stica
	- Objetivos 
		- Obtener modelo poblacional mediante sus par치metros. 
		- Medir la exactitud del modelo obtenido y sus estimadores. 
	- M칠todos de inferencia
		- Cl치sicos
		- Computacionales
			- Monte-Carlo
			- BootStrappping
			- Estimadores bayesianos 
Estimaci칩n param칠trica 
	- Un estimador:  Es un estad칤stico que se deben acercar al par치metro. 
	- Formas de evaluar un estimador: 
		- Sesgo
		- Error cuadr치tico medio
		- Eficiencia relativa 
		- Error est치ndar 
- Diferencias entre estad칤stica cl치sica vs. Computacional: 
	- Tama침o de las muestras
	- Distribuci칩n de los datos (homog칠neos vs. No homog칠neos) 
	- Manualmente tratable vs. Computacionalmente tratable
	- Algoritmos sencillos vs. Algoritmos complicados
- Modelizaci칩n: 
	- El objetivo de un modelo es explicar los datos y que sea capaz de predecir, y podamos sacar conclusiones y tomar decisiones. 
	- Debemos encontrar los par치metros que mejor representen a la poblaci칩n.
	- Hiperpar치metros: par치metros propios del modelo. 
- Partici칩n de datos 
	- Training sets: Entrenar el modelo (70%)
	- Validation sets: Si el modelo tiene hiperpar치metros, evaluar칤amos y seleccionar칤amos los valores 칩ptimos de los hiperpar치metros. (15%)
	- Test sets: Se usan para probar el rendimiento de nuestro modelo con datos que no se usaron para el entrenamiento (10-20%)
- Validaci칩n cruzada (k-fold validation): 
	- Se usa para determinar el error predictivo del modelo. 
	- Consiste en dividir los datos disponibles en k particiones. En la primera iteraci칩n se utilizan k-1 particiones para entrenar el modelo y la k-es칤ma partici칩n se usa para probar el modelo. En la segunda iteraci칩n utilizamosuna k-sima partici칩n diferente a la primera iteraci칩n y se repite el proceso. Iteramos k-veces, es decir, hasta que todas las particiones hayan pasado como test sets. 
	- Este es el modelo que escogemos para seleccionar los hiperpar치metros del modelo. 

## Estad칤stica Computacional e Introducci칩n al Aprendizaje Autom치tico 

Los problemas de ML y aprendizaje autom치tico se dividen en dos: 
- Supervisados: para cada valor de una variable entrada tenemos otra de salida. 
	- Predicci칩n 
	- Clasificaci칩n 
- No supervisados: no tenemos una variable respuesta asociada
- Metodos de Monte Carlo para la inferencia: siguen una aproximaci칩n param칠trica, es decir, que los datos analizados siguen una distribuci칩n con par치metros conocidos 
	- Se basan en la generaci칩n de un gran n칰mero de muestras aleatorias, por lo que requieren de gran potencia computacional. 
- M칠todos bootstrapping: Siguen una aproximaci칩n no param칠trica. No se asume que los datos siguen una distribuci칩n conocida. 
- Aprendizaje bayesiano: Se usa el teorema de Bayes para actualizar la probabilidad de una hip칩tesis a medida que tenemos m치s informaci칩n o evidencias . 
- Modelan la distribuci칩n de incertidumbre de los valores de los par치metros desconocidos como si fuera una probabilidad. 
	- Es excelente para: 
		- Reconocimiento de patrones
- Funciones de p칠rdidas
	- Tambi칠n llamada funci칩n de costes, es una forma de cuantificar lo bien que funciona un modelo. 
    - A veces se usa el t칠rmino funci칩n de p칠rdida para referirse a un solo experim ent o, mientras que se deja el t칠rmino funci칩n de coste para referirse al conjunto de datos. Esto es, la funci칩n de coste ser칤a el sumatorio de las funciones de p칠rdidas de cada uno de los datos.
	- Si el modelo se ajusta bien a los datos, el valor de la funci칩n de coste ser치 bajo. 
	- La funci칩n de p칠rdidas es la funci칩n objetivo a minimizar por el algoritmo optimizador  en el algoritmo de ML. 
		- Funciones de p칠rdida para regresi칩n 
			- Error cuadr치tico medio  (MSE): es la media artim칠tica de la diferencia al cuadrado del valor real de la variable respuesta y el valor predicho. Intensifica la importancia de los valores at칤picos. 
				- Error Cuadr치tico Medio Modificado: MSE/2
			- Error medio absoluto (MAE): Es parecido al anterior pero en lugar de la diferencia al cuadrado, es la diferencia absoluta. Es dif칤cil calcular su derivada. 
			- Error medio absoluto suavizado (Huber loss): funci칩n a trozos, introduce un nuevo hiperpar치metro. La elecci칩n de este par치metro es cr칤tico porque determinar치 qu칠 valores son outliers. 
		- Funciones de p칠rdida para clasificaci칩n 
			- Cross-entropy: 
				- Funci칩n usada en regresi칩n log칤stica
				- Se usa para problemas de clasificaci칩n binarias. 
		- Hinge Loss
			-  Se usa papra algoritmos llamados Support Vector Machine
			-  Se puede usar para problemas de clasificaci칩n categ칩ricas

## Regresi칩n lineal univariable
-  Minimizar la funci칩n de coste
	- Para encontrar el m칤nimo de una funci칩n, hay que hallar la derivada de esta e igualarla a cero. Observando la f칩rmula de la funci칩n de coste, vemos que tiene forma de paraboloide.
	- Existen diversos m칠todos para minimizar la funci칩n de coste; el m치s com칰n para ML es el del descenso del gradiente
- Descenso del Gradiente
	- Empezar con un valor inicial de ( 洧랚0, 洧랚1
	- Calcular el valor de las derivadas parciales de J para dichos valores y actualizar los valores ( 洧랚0, 洧랚1 ).
	- Repetir hasta que el valor de J no var칤e y hayamos encontrado su
		m칤nimo.
	- para los problemas de regresi칩n lineal, la funci칩n de coste siempre tendr치 forma convexa, por tanto, el algoritmo del descenso del gradiente siempre va a converger al m칤nimo absoluto.

## Regresi칩n multivariable y no lineal

- Un modelo multivariable es aquel en el que la variable respuesta depende de varias variable independientes o de entrada, tambi칠n llamadas variables explicativas.
- Las variables representan distintas magnitudes, por lo que su rango de
valores puede ser muy distinto.
- colinealidad 
    - Una de las condiciones para que el modelo de regresi칩n m칰ltiple funcione correctamente es que las variables de entrada sean independientes entre s칤. Cuando existe una gran correlaci칩n entre dos variables,  este hecho puede dar lugar a problemas de colinealidad.
- Regresi칩n no lineal: A la hora de formular el problema
no hay diferencia entre el problema lineal multivariable y el problema
no lineal, ya que aplicaremos una funci칩n a las variables no lineales para transformarlas en variables lineales. 
- Coeficiente de determinaci칩n 
    - Es un estad칤stico que nos da una medida de la bondad del modelo ajustado por regresi칩n. Se define como el ratio entre la variabilidad explicada por el modelo y la variabilidad total, y se denota por R2.
    - Se pueden comparar distintos modelos entre s칤 mediante el coeficiente de determinaci칩n de cada modelo. 
    - Cuanto mayor sea el valor de R2 mayor es la capacidad explicativa del modelo. Por tanto.

## Regresi칩n log칤stica
- 
- Se pueden construir modelos de clasificaci칩n binaria usando el modelo de regresi칩n log칤stica.
- El problema de clasificaci칩n puede verse as칤 como un problema de predicci칩n en el que hay que predecir la etiqueta de la variable de salida.
- Superficie de decisi칩n: es una divisi칩n del espacio, en donde cada regi칩n del espacio pertenece a una clase. 
- M칠todos de optimizaci칩n 
    - M치xima verosimilitud 
    - Descenso de gradiente
    - Optimizaci칩n avanzada

## Regularizaci칩n y selecci칩n del modelo

- El error de predicci칩n puede descomponerse como la suma de la varianza del modelo poblacional, m치s el sesgo al cuadrado del modelo estimado m치s la varianza del modelo estimado. 
- Para que el error de predicci칩n sea peque침o, debemos conseguir que tanto el
sesgo como la varianza del modelo sean peque침os.
- El sesgo es una medida de la diferencia entre el valor esperado de un par치metro y su valor real. Un modelo con un sesgo elevado predecir치 valores muy alejados de los reales.
- La varianza nos da idea de la dispersi칩n de nuestros datos con respecto a la media. 
- Equilibrio entre sesgo y varianza
    - Cuando un modelo produce un ajuste insuficiente de los datos, se dice que tiene un sesgo elevado y una varianza baja.
    - un modelo que est치 sobreajustado tiene una varianza muy alta y un sesgo normalmente bajo.
- Regularizaci칩n: El m칠todo de regularizaci칩n consiste en construir un modelo lineal complejo, que a priori tenga en cuenta todas las variables de entrada pero
penalizando los valores altos de los coeficientes, forzando a que varios de ellos tengan valores pr칩ximos a cero o incluso iguales a cero. 
    - Esta t칠cnica provoca un peque침o aumento en el sesgo pero, a cambio, consigue una notable reducci칩n en la varianza. 
    - Hay varios tipos de regularizaci칩n. Algunas de las m치s usadas son la regularizaci칩n Ridge, la Lasso y elastic net. 
    - Regularizaci칩n Ridge: Es una de las formas de regularizaci칩n m치s sencillas. Consiste en a침adir un t칠rmino de penalizaci칩n a la funci칩n de coste, que es igual al cuadrado de la suma de los coeficientes.
    - Regularizaci칩n Lasso: Este m칠todo de regularizaci칩n es muy parecido al anterior, con la salvedad de que el t칠rmino de penalizaci칩n que se a침ade a la funci칩n de coste es la suma de los valores absolutos de los coeficientes del modelo.

# An치lisis y modelos avanzados

## Estimaci칩n de funciones de densidad de probabilidad

- Se puede distinguir entre dos grandes grupos de estimaci칩n de funciones de densidad de probabilidad:
    - 1.Estimaci칩n param칠trica:  la funci칩n de densidad es estimada asumiendo un modelo de distribuci칩n conocido (por ej emplo, gaussiano, binomial, etc.), y utilizando los datos disponibles para ajustar los par치metros del modelo elegido. El problema ser치 determinar dichos par치metros.
    - 2. Estimaci칩n no param칠trica o semiparam칠trica: hace uso de t칠cnicas que no hacen suposiciones (o hacen muy pocas y d칠biles) sobre la forma que tiene la funci칩n de densidad de probabilidad.
- m칠todos para calcular el error cometido:
    - el error cuadr치tico medio integrado o MISE (del ingl칠s mean integrated squared error)
    -  el error medio absoluto integrado o MIAE (del ingl칠s, mean integrated absolute error).
- ancho de bin h determina c칩mo de suave es la forma del histograma, por lo que recibe el nombre de par치metro de suavizado.
- Estimaci칩n por histogramas
    - Pol칤gonos de frecuencias: son un m칠todo para estimar funciones de densidad de probabilidad, que se basan tambi칠n en histogramas. La funci칩n de densidad de probabilidad se determina por interpolaci칩n lineal entre los puntos medios de cada una de las barras (o bins) del histograma. Es decir, primero se obtendr치 el histograma, y luego se calcular치 el valor de la funci칩n de distribuci칩n en base a esta expresi칩n. 
    - M칠todo ASH: El m칠todo ASH (del ingl칠s average shifted histogram) surge para tener en cuenta el origen t0. La idea detr치s de este m칠todo es crear varios histogramas con igual ancho h y distinto origen t0 y promediarlos. De esta forma se consigue un estimador de la funci칩n de densidad m치s suave.
- Estimaci칩n Kernel: Normalmente se elige como funci칩n kernel una distribuci칩n de probabilidad conocida, siendo muy habitual elegir la normal est치ndar. Otro par치metro es el ancho de bin, h. 


## An치lisis de series temporales 

- Una serie temporal es una sucesi칩n de observaciones de una variable, o un conjunto de datos, que han sido recogidas de forma secuencial en distintos instantes de tiempo.
- El objetivo del an치lisis de series temporales es estudiar el comportamiento de la variable con respecto al tiempo para encontrar posibles patrones y para ser capaces de predecir valores futuros de dicha variable.
- El objetivo principal del an치lisis de una serie temporal es la predicci칩n de valores futuros, aunque tambi칠n interesa analizar la serie en busca de patrones, o para analizar las razones de posibles cambios en el tiempo.
- Representaci칩n de series temporales
    - Gr치ficas temporales, barras, velas
- Clasificiaci칩n de series temporales: 
    - Si la serie temporal es tal que se puede determinar exactamente un valor futuro, se dice que la serie es determinista. Si, por el contrario, el valor futuro **no se puede predecir con certeza, se dice que la serie es estoc치stica**.
    - Series temporales estacionarias: su media y su variabilidad se mantienen
constantes a lo largo del tiempo.
        - Siempre es deseable que las series temporales sean estacionarias porque, de esta forma, es m치s f치cil hacer predicciones. Como la media es constante, esta puede predecirse usando todos los datos disponibles y usarse para predecir nuevas observaciones.
    - Series temporales no estacionarias: 
        - Cambios en la varianza
        - Son estacionales
        - Tienen tendencias. 
    - An치lisis de series temporales: una serie temporal puede expresarse como la suma de varias componentes b치sicas, que son la tendencia, la estacionalidad y un t칠rmino irregular o aleatorio. 
        - Componentes de una serie temporal no estacionaria: 
            - Tendencia: el cambio a largo plazo de la media
            - Estacionalidad: Pueden presentar estacionalidad. 
            - Aleatoriedad: ruido. 
        -Con el objeto de analizar la serie temporal y conocer su comportamiento a largo plazo, es necesario aislar de alguna manera la componente irregular para poder describirla con el modelo probabil칤stico m치s adecuado. 
            - Enfoque descriptivo: se estiman tanto la tendencia como la estacionalidad, quedando solo la aleatoriedad. 
            - Enfoque Box-Jenkins: Usando transformaciones y filtros se eliminan tanto la tendencia como la estacionalidad de la serie temporal. 
        - Heterocedasticidad: que la varianza aumente de forma proporcional en el tiempo.
        - Estimaci칩n de la tendencia: 
            - Relaci칩n determinista: En algunas ocasiones se puede expresar la relaci칩n entre la tendencia y el tiempo de forma lineal. 
            - Tendencia evolutiva: consiste en suponer que la tendencia evoluciona de forma suave en el tiempo y que en los intervalos cortos de tiempo se puede aproximar medias m칩viles. 
                - Cuanto mayor sea el orden de la media m칩vil mayor ser치 el suavizado, pero se perder치n m치s datos en el proceso. 
            - Diferenciaci칩n de la serie: Consiste en suponer que la tendencia evoluciona lentamente en el tiempo y no asumir ninguna forma determinada. 
        - Estimaci칩n de la estacionalidad: 
            - Coeficientes estacionales: se calculan como la media de las observaciones del periodo correspondiente, menos la media global de todo el periodo. 
                - Una vez calculado el coeficiente estacional, se desestacionaliza la serie restando a cada observaci칩n mensual su coeficiente estacional.
            - Diferenciaci칩n estacional: similar a la tendencia. 
    - Predicci칩n de series 
        - Es necesario tener muchos datos. 
        - Para predecir la componente estacional, normalmente se presupone que esta no
cambia en el tiempo o lo hace de forma muy lentamente, por lo que se toma el valor de la componente estimada para el a침o anterior.
        - Modelo Box-Jenkins Arima
            - Son modelos de an치lisis y predicci칩n 칰tiles para variables unidimensionales que dependen del tiempo. Estos modelos se basan en la hip칩tesis de que los datos son estacionarios, por tanto, en el caso en que la serie no lo sea, hay que efectuar antes el an치lisis y las transformaciones necesarias para convertirla en aproximadamente estacionaria. Para ello se hace uso de las herramientas vistas para la tendencia y la estacionalidad.
        - modelo de alisado exponencial: Se basan en modelos param칠tricos deterministas que se ajustan a la evoluci칩n de la serie. Se les dan distintos pesos a las observaciones, de forma que las observaciones m치s recientes tienen m치s peso que las m치s alejadas en el tiempo. Los pesos caen de forma exponencial a medida que las observaciones se hacen m치s viejas, de ah칤 su nombre
