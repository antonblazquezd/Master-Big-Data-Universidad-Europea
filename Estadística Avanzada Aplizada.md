
Elena Igualada Villodre
Ingeniera Industrial 
Doctora en Matem치ticas


4 actividades diferentes (10%)
1. Cuestionario tipo test. 
3 pr치cticas en Python. 


5 semanas de asignatura
Examen en marzo 


# Contenidos

- Objetivos
    - Objetivos espec칤ficos
    - Resultados de aprendizaje
- Unidad 1. Variables y Estimadores
    - Teor칤a de probabilidad y estad칤stica b치sica
    - T칠cnicas de descripci칩n y modelado de datos complejos: muestreo, training, test set
    - Introducci칩n al aprendizaje estad칤stico: inferencia, aprendizaje bayesiano, funci칩n de p칠rdida
- Unidad 2. Modelos de Regresi칩n y regularizaci칩n
    - Regresi칩n lineal, no lineal y log칤stica
    - Regresi칩n con m칰ltiples variables
    - Regularizaci칩n: ridge y lasso
    - Regresi칩n no param칠trica: spline y kernel.
Unidad 3. An치lisis y modelado avanzados
    - 츼rboles de decisi칩n. Modelos gr치ficos probabilistas.
    - Modelado de funciones de densidad de probabilidad
    - An치lisis de Series temporales
    - Optimizaci칩n para grandes vol칰menes de datos

# Objetivos

## Objetivos espec칤ficos
- Dise침ar y aplicar algoritmos de an치lisis basados en sistemas e infraestructuras de almacenamiento y acceso a grandes vol칰menes de datos. 

## Resultados de aprendizaje

- Comparaci칩n y contraste cr칤ticos de m칰ltiples modelos de sistemas distribuidos y sus tecnolog칤as habilitantes asociadas.

# Unidad 1. Variables y Estimadores

## Probabilidad y variables estimatorias

- Espacio muestral: Al conjunto de todos los escenarios posibles de un experimento .
- Suceso elemental: Cada uno de los posibles resultados de un experimento contenido en un espacio muestral. 
- Suceso compuesto: subconjunto del espacio muestral o una agrupaci칩n de espacios muestrales. 
- Sucesos disjuntos o mutuamente excluyentes: si pasa uno el otro no puede pasar. 
- Enfoque frecuentista de la probabilidad: cree que la probabilidad de que los sucesos elementales sucedan es igual. Cree que en la tendencia a un resultado si lo repetimos una y otra vez. 
- Enfoque axiom치tico: cree que los sucesos elementales no son igualmente probables de ocurrir. Considera en que la suma de todas las probabilidades de los sucesos elementales debe ser igual a 1. 
- Variable aleatoria: mide una propiedad de la 
- Variables aleatorias discretas --> funci칩n de masa 
- Variables aleatorias continuas --> funci칩n de densidad  
- Modelos probabil칤sticos t칤picos: 
	- Binomial: Dos sucesos elementales: 칠xito/fracaso.
	- Poisson: modelar las probabilidades un tiempo determinado 
	- Exponencial 
	- Uniforme
	-  Modelo normal: sigue el teorema central del l칤mite. 



## Teor칤a de muestreo y partici칩n de datos

- Metodolog칤a para el an치lisis de datos: 
	- Preparaci칩n de los datos
		- Recogida
		- Limpieza de los datos (eliminar datos duplicados, errores, campos vac칤os y nulos).
		- Transformaci칩n de variables categ칩ricas en num칠ricas. 
	- An치lisis estad칤stico
		- Estad칤stica descriptiva: Aplicamos un an치lisis exploratorio de datos (EDA), sin preguntarnos por la poblaci칩n ni hacer inferenc. 
		- An치lisis estad칤stico: se buscan modelos que se ajusten a nuestros datos
		- Aprendizaje sobre el fen칩meno
		- Creamos un modelo que sea capaz de predecir el comportamiento de una variable futura. 
Teor칤a de muestreo 
	- Muestra vs. Poblaci칩n 
		- Poblaci칩n -objetivo: es un subconjunto del universo que engloba a toda la poblaci칩n que cumple las condiciones para realizar un experimento. 
		- Muestra: una parte representativa de la poblaci칩n. 
	- Distribuci칩n muestral 
		- Necesitamos conocer el nivel de incertidumbre de nuestros estad칤sticos y para ello debemos conocer la distribuci칩n muestral
	- Par치metros vs Estad칤stico
		- Par치metro: es una medida descriptiva de una poblaci칩n
		- Estad칤stico: es una medida descriptiva de la muestra. 
- Inferencia estad칤stica
	- Objetivos 
		- Obtener modelo poblacional mediante sus par치metros. 
		- Medir la exactitud del modelo obtenido y sus estimadores. 
	- M칠todos de inferencia
		- Cl치sicos
		- Computacionales
			- Monte-Carlo
			- BootStrappping
			- Estimadores bayesianos 
Estimaci칩n param칠trica 
	- Un estimador:  Es un estad칤stico que se deben acercar al par치metro. 
	- Formas de evaluar un estimador: 
		- Sesgo
		- Error cuadr치tico medio
		- Eficiencia relativa 
		- Error est치ndar 
- Diferencias entre estad칤stica cl치sica vs. Computacional: 
	- Tama침o de las muestras
	- Distribuci칩n de los datos (homog칠neos vs. No homog칠neos) 
	- Manualmente tratable vs. Computacionalmente tratable
	- Algoritmos sencillos vs. Algoritmos complicados
- Modelizaci칩n: 
	- El objetivo de un modelo es explicar los datos y que sea capaz de predecir, y podamos sacar conclusiones y tomar decisiones. 
	- Debemos encontrar los par치metros que mejor representen a la poblaci칩n.
	- Hiperpar치metros: par치metros propios del modelo. 
- Partici칩n de datos 
	- Training sets: Entrenar el modelo (70%)
	- Validation sets: Si el modelo tiene hiperpar치metros, evaluar칤amos y seleccionar칤amos los valores 칩ptimos de los hiperpar치metros. (15%)
	- Test sets: Se usan para probar el rendimiento de nuestro modelo con datos que no se usaron para el entrenamiento (10-20%)
- Validaci칩n cruzada (k-fold validation): 
	- Se usa para determinar el error predictivo del modelo. 
	- Consiste en dividir los datos disponibles en k particiones. En la primera iteraci칩n se utilizan k-1 particiones para entrenar el modelo y la k-es칤ma partici칩n se usa para probar el modelo. En la segunda iteraci칩n utilizamosuna k-sima partici칩n diferente a la primera iteraci칩n y se repite el proceso. Iteramos k-veces, es decir, hasta que todas las particiones hayan pasado como test sets. 
	- Este es el modelo que escogemos para seleccionar los hiperpar치metros del modelo. 

## Estad칤stica Computacional e Introducci칩n al Aprendizaje Autom치tico 

Los problemas de ML y aprendizaje autom치tico se dividen en dos: 
- Supervisados: para cada valor de una variable entrada tenemos otra de salida. 
	- Predicci칩n 
	- Clasificaci칩n 
- No supervisados: no tenemos una variable respuesta asociada
- Metodos de Monte Carlo para la inferencia: siguen una aproximaci칩n param칠trica, es decir, que los datos analizados siguen una distribuci칩n con par치metros conocidos 
	- Se basan en la generaci칩n de un gran n칰mero de muestras aleatorias, por lo que requieren de gran potencia computacional. 
- M칠todos bootstrapping: Siguen una aproximaci칩n no param칠trica. No se asume que los datos siguen una distribuci칩n conocida. 
- Aprendizaje bayesiano: Se usa el teorema de Bayes para actualizar la probabilidad de una hip칩tesis a medida que tenemos m치s informaci칩n o evidencias . 
- Modelan la distribuci칩n de incertidumbre de los valores de los par치metros desconocidos como si fuera una probabilidad. 
	- Es excelente para: 
		- Reconocimiento de patrones
- Funciones de p칠rdidas
	- Tambi칠n llamada funci칩n de costes, es una forma de cuantificar lo bien que funciona un modelo. 
	- Si el modelo se ajusta bien a los datos, el valor de la funci칩n de coste ser치 bajo. 
	- La funci칩n de p칠rdidas es la funci칩n objetivo a minimizar por el algoritmo optimizador  en el algoritmo de ML. 
		- Funciones de p칠rdida para regresi칩n 
			- Error cuadr치tico medio  (MSE): es la media artim칠tica de la diferencia al cuadrado del valor real de la variable respuesta y el valor predicho. Intensifica la importancia de los valores at칤picos. 
				- Error Cuadr치tico Medio Modificado: MSE/2
			- Error medio absoluto (MAE): Es parecido al anterior pero en lugar de la diferencia al cuadrado, es la diferencia absoluta. Es dif칤cil calcular su derivada. 
			- Error medio absoluto suavizado (Huber loss): funci칩n a trozos, introduce un nuevo hiperpar치metro. La elecci칩n de este par치metro es cr칤tico porque determinar치 qu칠 valores son outliers. 
		- Funciones de p칠rdida para clasificaci칩n 
			- Cross-entropy: 
				- Funci칩n usada en regresi칩n log칤stica
				- Se usa para problemas de clasificaci칩n binarias. 
		- Hinge Loss
			-  Se usa papra algoritmos llamados Support Vector Machine
			-  Se puede usar para problemas de clasificaci칩n categ칩ricas

## Regresi칩n lineal univariable
-  Minimizar la funci칩n de coste
	- Para encontrar el m칤nimo de una funci칩n, hay que hallar la derivada de esta e igualarla a cero. Observando la f칩rmula de la funci칩n de coste, vemos que tiene forma de paraboloide.
	- Existen diversos m칠todos para minimizar la funci칩n de coste; el m치s com칰n para ML es el del descenso del gradiente
- Descenso del Gradiente
	- Empezar con un valor inicial de ( 洧랚0, 洧랚1
	- Calcular el valor de las derivadas parciales de J para dichos valores y actualizar los valores ( 洧랚0, 洧랚1 ).
	- Repetir hasta que el valor de J no var칤e y hayamos encontrado su
		m칤nimo.
	- para los problemas de regresi칩n lineal, la funci칩n de coste siempre tendr치 forma convexa, por tanto, el algoritmo del descenso del gradiente siempre va a converger al m칤nimo absoluto.
